# Literature Review

## Another Larger Repository of Papers

https://github.com/WhileBug/AwesomeLLMJailBreakPapers

## Key Reference

### Safeguarding Large Language Models: A Survey
https://arxiv.org/pdf/2406.02622

## Potential References

### JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks
https://arxiv.org/pdf/2404.03027v4

### Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective
https://aclanthology.org/2025.coling-main.212.pdf

## Key Videos

### Learn to Implement Guardrails in Generative AI Applications
https://www.youtube.com/watch?v=XKDcOi-rZ_I

This video demonstrates how to implement simple guardrails using Guardrails AI.

Another useful source based from this video is the Guardrails AI GitHub page:

https://github.com/guardrails-ai/guardrails

### Guardrails Crash Course for Beginners
https://www.youtube.com/watch?v=XbriX2aYgqw

A YouTube tutorial detailing a small "crash course" for beginners on using NeMo Guardrails, Llama Guard, and Guardrails AI.

This video comes with an associated GitHub page that gives examples of usages:

https://github.com/AIAnytime/Guardrails-Crash-Course

### Build safe and reliable LLM applications with guardrails in this new course
https://www.youtube.com/watch?v=6mNnPISidlg

This video is an introduction to an online course, which is the more useful source:

https://www.deeplearning.ai/short-courses/safe-and-reliable-ai-via-guardrails/
